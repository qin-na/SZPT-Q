{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qin-na/SZPT-Q/blob/main/%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4XOeGVQwKyn",
        "outputId": "e8ab0680-6dfe-4100-fe50-08b5dd50520e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate>=0.12.0 (from -r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 1))\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval (from -r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 2))\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets>=1.8.0 (from -r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3))\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 4)) (2.1.0+cu121)\n",
            "Collecting evaluate (from -r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 5))\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 2)) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (3.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 4)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 4)) (2.1.0)\n",
            "Collecting responses<0.19 (from evaluate->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 5))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 2)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt (line 3)) (1.16.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=b30b29f5dfdb08a8f47d9e1589d7006f2c43923c3bba0afb270960f328c212a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: dill, responses, multiprocess, seqeval, accelerate, datasets, evaluate\n",
            "Successfully installed accelerate-0.27.2 datasets-2.17.1 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0 seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/drive/MyDrive/transformers/examples/pytorch/token-classification/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66UziGPVwPTU",
        "outputId": "ee1b5efc-8fff-4fa0-c492-21dd9e016d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX7JZs5m2tdw"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.34.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2QV37KiwJEb",
        "outputId": "d4391f48-1058-4d04-f4e8-adf8b9338747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-27 08:30:41.216829: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-27 08:30:41.216898: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-27 08:30:41.216942: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-27 08:30:43.046880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/27/2023 08:30:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/27/2023 08:30:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/model_use123/runs/Oct27_08-30-48_8898d633d861,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/drive/MyDrive/model_use123,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/model_use123,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-a832c5da87e157f2\n",
            "10/27/2023 08:30:50 - INFO - datasets.builder - Using custom data configuration default-a832c5da87e157f2\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "10/27/2023 08:30:50 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "10/27/2023 08:30:50 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "10/27/2023 08:30:50 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 3851.52it/s]\n",
            "Downloading took 0.0 min\n",
            "10/27/2023 08:30:50 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "10/27/2023 08:30:50 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:03<00:00,  1.52s/it]\n",
            "Generating train split\n",
            "10/27/2023 08:30:53 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 126 examples [00:00, 153.94 examples/s]\n",
            "Generating validation split\n",
            "10/27/2023 08:30:54 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 42 examples [00:00, 8142.02 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "10/27/2023 08:30:54 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "10/27/2023 08:30:54 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Downloading (…)lve/main/config.json: 100% 717/717 [00:00<00:00, 3.37MB/s]\n",
            "[INFO|configuration_utils.py:715] 2023-10-27 08:30:55,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-10-27 08:30:55,327 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:550] 2023-10-27 08:30:55,581 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2023-10-27 08:30:55,843 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-10-27 08:30:55,844 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Downloading (…)solve/main/vocab.txt: 100% 226k/226k [00:00<00:00, 26.2MB/s]\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-10-27 08:30:57,368 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-10-27 08:30:57,369 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-10-27 08:30:57,369 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-10-27 08:30:57,369 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-10-27 08:30:57,369 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:715] 2023-10-27 08:30:57,369 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-10-27 08:30:57,370 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:715] 2023-10-27 08:30:57,403 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-10-27 08:30:57,404 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 438M/438M [00:26<00:00, 16.3MB/s]\n",
            "[INFO|modeling_utils.py:2993] 2023-10-27 08:31:26,471 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3765] 2023-10-27 08:31:27,744 >> Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3777] 2023-10-27 08:31:27,744 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/126 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b894d2f032f327f0.arrow\n",
            "10/27/2023 08:31:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b894d2f032f327f0.arrow\n",
            "Running tokenizer on train dataset: 100% 126/126 [00:00<00:00, 457.88 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/42 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e77cd02745bfb9e.arrow\n",
            "10/27/2023 08:31:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-a832c5da87e157f2/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e77cd02745bfb9e.arrow\n",
            "Running tokenizer on validation dataset: 100% 42/42 [00:00<00:00, 503.59 examples/s]\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 17.3MB/s]\n",
            "[INFO|trainer.py:761] 2023-10-27 08:31:36,046 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1760] 2023-10-27 08:31:36,056 >> ***** Running training *****\n",
            "[INFO|trainer.py:1761] 2023-10-27 08:31:36,056 >>   Num examples = 126\n",
            "[INFO|trainer.py:1762] 2023-10-27 08:31:36,056 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1763] 2023-10-27 08:31:36,056 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1766] 2023-10-27 08:31:36,056 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1767] 2023-10-27 08:31:36,056 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1768] 2023-10-27 08:31:36,056 >>   Total optimization steps = 48\n",
            "[INFO|trainer.py:1769] 2023-10-27 08:31:36,057 >>   Number of trainable parameters = 108,895,493\n",
            "  0% 0/48 [00:00<?, ?it/s][WARNING|logging.py:290] 2023-10-27 08:31:36,078 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 48/48 [00:31<00:00,  1.83it/s][INFO|trainer.py:2017] 2023-10-27 08:32:07,889 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 31.8635, 'train_samples_per_second': 11.863, 'train_steps_per_second': 1.506, 'train_loss': 0.23928908507029215, 'epoch': 3.0}\n",
            "100% 48/48 [00:31<00:00,  1.51it/s]\n",
            "[INFO|trainer.py:2939] 2023-10-27 08:32:07,927 >> Saving model checkpoint to /content/drive/MyDrive/model_use123\n",
            "[INFO|configuration_utils.py:460] 2023-10-27 08:32:07,934 >> Configuration saved in /content/drive/MyDrive/model_use123/config.json\n",
            "[INFO|modeling_utils.py:2118] 2023-10-27 08:32:09,410 >> Model weights saved in /content/drive/MyDrive/model_use123/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-10-27 08:32:09,415 >> tokenizer config file saved in /content/drive/MyDrive/model_use123/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-10-27 08:32:09,418 >> Special tokens file saved in /content/drive/MyDrive/model_use123/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.2393\n",
            "  train_runtime            = 0:00:31.86\n",
            "  train_samples            =        126\n",
            "  train_samples_per_second =     11.863\n",
            "  train_steps_per_second   =      1.506\n",
            "10/27/2023 08:32:09 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:761] 2023-10-27 08:32:09,456 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3213] 2023-10-27 08:32:09,458 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3215] 2023-10-27 08:32:09,458 >>   Num examples = 42\n",
            "[INFO|trainer.py:3218] 2023-10-27 08:32:09,458 >>   Batch size = 8\n",
            " 83% 5/6 [00:00<00:00,  4.97it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CATALYST seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPERTY_NAME seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPERTY_VALUE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CO-CATALYST seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100% 6/6 [00:01<00:00,  5.20it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.9587\n",
            "  eval_f1                 =     0.6384\n",
            "  eval_loss               =     0.1406\n",
            "  eval_precision          =     0.6893\n",
            "  eval_recall             =     0.5944\n",
            "  eval_runtime            = 0:00:01.37\n",
            "  eval_samples            =         42\n",
            "  eval_samples_per_second =     30.452\n",
            "  eval_steps_per_second   =       4.35\n",
            "[INFO|modelcard.py:452] 2023-10-27 08:32:11,127 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.6893491124260355}, {'name': 'Recall', 'type': 'recall', 'value': 0.5943877551020408}, {'name': 'F1', 'type': 'f1', 'value': 0.6383561643835616}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9587407584068686}]}\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/transformers/examples/pytorch/token-classification/run_ner.py \\\n",
        "  --model_name_or_path pranav-s/MaterialsBERT \\\n",
        "  --train_file /content/drive/MyDrive/data/train126.json \\\n",
        "  --validation_file /content/drive/MyDrive/data/test42.json \\\n",
        "  --output_dir /content/drive/MyDrive/model_use123 \\\n",
        "  --do_train \\\n",
        "  --do_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQkhMql1xuiZ",
        "outputId": "a9258c51-8aee-4da0-f26f-2bb3cc54b68c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-21 17:48:07.125258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-21 17:48:07.125311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-21 17:48:07.126654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-21 17:48:08.211741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "02/21/2024 17:48:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "02/21/2024 17:48:11 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/catalystNER/runs/Feb21_17-48-11_f84d4cd34028,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/drive/MyDrive/catalystNER,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/catalystNER,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-254f1f58fa09527d\n",
            "02/21/2024 17:48:12 - INFO - datasets.builder - Using custom data configuration default-254f1f58fa09527d\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "02/21/2024 17:48:12 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "02/21/2024 17:48:12 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "02/21/2024 17:48:12 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading took 0.0 min\n",
            "02/21/2024 17:48:12 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "02/21/2024 17:48:12 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "02/21/2024 17:48:14 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 210 examples [00:00, 6673.16 examples/s]\n",
            "Generating validation split\n",
            "02/21/2024 17:48:14 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 42 examples [00:00, 5984.13 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "02/21/2024 17:48:14 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "02/21/2024 17:48:14 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "config.json: 100% 717/717 [00:00<00:00, 3.74MB/s]\n",
            "[INFO|configuration_utils.py:729] 2024-02-21 17:48:15,044 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-21 17:48:15,053 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:606] 2024-02-21 17:48:15,316 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:729] 2024-02-21 17:48:15,580 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-21 17:48:15,581 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "vocab.txt: 100% 226k/226k [00:00<00:00, 506kB/s]\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-21 17:48:17,672 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-21 17:48:17,673 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-21 17:48:17,673 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-21 17:48:17,673 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-21 17:48:17,673 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:729] 2024-02-21 17:48:17,673 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-21 17:48:17,674 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:729] 2024-02-21 17:48:17,728 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-21 17:48:17,729 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "pytorch_model.bin: 100% 438M/438M [00:28<00:00, 15.6MB/s]\n",
            "[INFO|modeling_utils.py:3476] 2024-02-21 17:48:48,603 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "[INFO|modeling_utils.py:4340] 2024-02-21 17:48:48,998 >> Some weights of the model checkpoint at pranav-s/MaterialsBERT were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:4352] 2024-02-21 17:48:48,998 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at pranav-s/MaterialsBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/210 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-afc58f258bfc73b5.arrow\n",
            "02/21/2024 17:48:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-afc58f258bfc73b5.arrow\n",
            "Running tokenizer on train dataset: 100% 210/210 [00:00<00:00, 510.83 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/42 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d2d614890712b25.arrow\n",
            "02/21/2024 17:48:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-1d2d614890712b25.arrow\n",
            "Running tokenizer on validation dataset: 100% 42/42 [00:00<00:00, 514.42 examples/s]\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 18.6MB/s]\n",
            "[INFO|trainer.py:718] 2024-02-21 17:48:52,034 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1721] 2024-02-21 17:48:52,049 >> ***** Running training *****\n",
            "[INFO|trainer.py:1722] 2024-02-21 17:48:52,049 >>   Num examples = 210\n",
            "[INFO|trainer.py:1723] 2024-02-21 17:48:52,049 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1724] 2024-02-21 17:48:52,049 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1727] 2024-02-21 17:48:52,049 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1728] 2024-02-21 17:48:52,049 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1729] 2024-02-21 17:48:52,049 >>   Total optimization steps = 81\n",
            "[INFO|trainer.py:1730] 2024-02-21 17:48:52,051 >>   Number of trainable parameters = 108,896,262\n",
            "100% 81/81 [00:53<00:00,  1.91it/s][INFO|trainer.py:1962] 2024-02-21 17:49:45,520 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 53.4955, 'train_samples_per_second': 11.777, 'train_steps_per_second': 1.514, 'train_loss': 0.19111914693573376, 'epoch': 3.0}\n",
            "100% 81/81 [00:53<00:00,  1.51it/s]\n",
            "[INFO|trainer.py:2936] 2024-02-21 17:49:45,551 >> Saving model checkpoint to /content/drive/MyDrive/catalystNER\n",
            "[INFO|configuration_utils.py:473] 2024-02-21 17:49:45,559 >> Configuration saved in /content/drive/MyDrive/catalystNER/config.json\n",
            "[INFO|modeling_utils.py:2493] 2024-02-21 17:49:47,422 >> Model weights saved in /content/drive/MyDrive/catalystNER/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2433] 2024-02-21 17:49:47,428 >> tokenizer config file saved in /content/drive/MyDrive/catalystNER/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2442] 2024-02-21 17:49:47,433 >> Special tokens file saved in /content/drive/MyDrive/catalystNER/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.1911\n",
            "  train_runtime            = 0:00:53.49\n",
            "  train_samples            =        210\n",
            "  train_samples_per_second =     11.777\n",
            "  train_steps_per_second   =      1.514\n",
            "02/21/2024 17:49:48 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:718] 2024-02-21 17:49:48,190 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3242] 2024-02-21 17:49:48,193 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3244] 2024-02-21 17:49:48,193 >>   Num examples = 42\n",
            "[INFO|trainer.py:3247] 2024-02-21 17:49:48,193 >>   Batch size = 8\n",
            " 83% 5/6 [00:00<00:00,  4.85it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CATALYST seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPERTY_NAME seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPERTY_VALUE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CO-CATALYST seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100% 6/6 [00:01<00:00,  5.13it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.9705\n",
            "  eval_f1                 =     0.7572\n",
            "  eval_loss               =     0.0857\n",
            "  eval_precision          =     0.7159\n",
            "  eval_recall             =     0.8036\n",
            "  eval_runtime            = 0:00:01.45\n",
            "  eval_samples            =         42\n",
            "  eval_samples_per_second =     28.889\n",
            "  eval_steps_per_second   =      4.127\n",
            "[INFO|modelcard.py:452] 2024-02-21 17:49:49,941 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.7159090909090909}, {'name': 'Recall', 'type': 'recall', 'value': 0.8035714285714286}, {'name': 'F1', 'type': 'f1', 'value': 0.7572115384615384}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9705461483424755}]}\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/transformers/examples/pytorch/token-classification/run_ner.py \\\n",
        "  --model_name_or_path pranav-s/MaterialsBERT \\\n",
        "  --train_file /content/drive/MyDrive/data/num210.json \\\n",
        "  --validation_file /content/drive/MyDrive/data/test42.json \\\n",
        "  --output_dir /content/drive/MyDrive/catalystNER \\\n",
        "  --do_train \\\n",
        "  --do_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRZpz0Tq1tIq",
        "outputId": "9eede576-ba7a-440b-f347-ffcf1ed61c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-22 14:30:22.747682: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-22 14:30:22.747740: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-22 14:30:22.749046: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-22 14:30:23.840551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "02/22/2024 14:30:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "02/22/2024 14:30:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/catalystNER/runs/Feb22_14-30-26_b8d56e31b3f1,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/catalystNER,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/catalystNER,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-254f1f58fa09527d\n",
            "02/22/2024 14:30:27 - INFO - datasets.builder - Using custom data configuration default-254f1f58fa09527d\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "02/22/2024 14:30:27 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "02/22/2024 14:30:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "02/22/2024 14:30:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "02/22/2024 14:30:27 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "02/22/2024 14:30:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "config.json: 100% 385/385 [00:00<00:00, 1.50MB/s]\n",
            "[INFO|configuration_utils.py:729] 2024-02-22 14:30:27,858 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-22 14:30:27,866 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:606] 2024-02-22 14:30:28,125 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:729] 2024-02-22 14:30:28,382 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-22 14:30:28,384 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "vocab.txt: 100% 228k/228k [00:00<00:00, 462kB/s]\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-22 14:30:30,730 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-22 14:30:30,730 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-22 14:30:30,730 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-22 14:30:30,730 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2027] 2024-02-22 14:30:30,730 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:729] 2024-02-22 14:30:30,730 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-22 14:30:30,731 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:729] 2024-02-22 14:30:30,767 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-22 14:30:30,768 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "pytorch_model.bin: 100% 442M/442M [00:06<00:00, 68.9MB/s]\n",
            "[INFO|modeling_utils.py:3476] 2024-02-22 14:30:38,187 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/pytorch_model.bin\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "[INFO|modeling_utils.py:4340] 2024-02-22 14:30:39,084 >> Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:4352] 2024-02-22 14:30:39,084 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/210 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2f49d869966715a1.arrow\n",
            "02/22/2024 14:30:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2f49d869966715a1.arrow\n",
            "Running tokenizer on train dataset: 100% 210/210 [00:00<00:00, 261.66 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/42 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7aad9995e40ca8ea.arrow\n",
            "02/22/2024 14:30:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-254f1f58fa09527d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7aad9995e40ca8ea.arrow\n",
            "Running tokenizer on validation dataset: 100% 42/42 [00:00<00:00, 275.81 examples/s]\n",
            "[INFO|trainer.py:718] 2024-02-22 14:30:42,427 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1721] 2024-02-22 14:30:42,441 >> ***** Running training *****\n",
            "[INFO|trainer.py:1722] 2024-02-22 14:30:42,441 >>   Num examples = 210\n",
            "[INFO|trainer.py:1723] 2024-02-22 14:30:42,441 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1724] 2024-02-22 14:30:42,441 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1727] 2024-02-22 14:30:42,441 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1728] 2024-02-22 14:30:42,441 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1729] 2024-02-22 14:30:42,441 >>   Total optimization steps = 81\n",
            "[INFO|trainer.py:1730] 2024-02-22 14:30:42,442 >>   Number of trainable parameters = 109,332,486\n",
            "100% 81/81 [00:51<00:00,  1.94it/s][INFO|trainer.py:1962] 2024-02-22 14:31:33,870 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 51.4601, 'train_samples_per_second': 12.242, 'train_steps_per_second': 1.574, 'train_loss': 0.12140623728434245, 'epoch': 3.0}\n",
            "100% 81/81 [00:51<00:00,  1.57it/s]\n",
            "[INFO|trainer.py:2936] 2024-02-22 14:31:33,904 >> Saving model checkpoint to /content/catalystNER\n",
            "[INFO|configuration_utils.py:473] 2024-02-22 14:31:33,905 >> Configuration saved in /content/catalystNER/config.json\n",
            "[INFO|modeling_utils.py:2493] 2024-02-22 14:31:35,250 >> Model weights saved in /content/catalystNER/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2433] 2024-02-22 14:31:35,251 >> tokenizer config file saved in /content/catalystNER/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2442] 2024-02-22 14:31:35,251 >> Special tokens file saved in /content/catalystNER/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.1214\n",
            "  train_runtime            = 0:00:51.46\n",
            "  train_samples            =        210\n",
            "  train_samples_per_second =     12.242\n",
            "  train_steps_per_second   =      1.574\n",
            "02/22/2024 14:31:35 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:718] 2024-02-22 14:31:35,266 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, ner. If words, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3242] 2024-02-22 14:31:35,268 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3244] 2024-02-22 14:31:35,268 >>   Num examples = 42\n",
            "[INFO|trainer.py:3247] 2024-02-22 14:31:35,268 >>   Batch size = 8\n",
            " 83% 5/6 [00:00<00:00,  4.81it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CATALYST seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPERTY_NAME seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPERTY_VALUE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CO-CATALYST seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100% 6/6 [00:01<00:00,  5.08it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.9792\n",
            "  eval_f1                 =     0.8264\n",
            "  eval_loss               =     0.0585\n",
            "  eval_precision          =     0.7934\n",
            "  eval_recall             =     0.8622\n",
            "  eval_runtime            = 0:00:01.41\n",
            "  eval_samples            =         42\n",
            "  eval_samples_per_second =     29.745\n",
            "  eval_steps_per_second   =      4.249\n",
            "[INFO|modelcard.py:452] 2024-02-22 14:31:37,065 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.7934272300469484}, {'name': 'Recall', 'type': 'recall', 'value': 0.8622448979591837}, {'name': 'F1', 'type': 'f1', 'value': 0.8264058679706602}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.979236276849642}]}\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/transformers/examples/pytorch/token-classification/run_ner.py \\\n",
        "  --model_name_or_path allenai/scibert_scivocab_uncased \\\n",
        "  --train_file /content/drive/MyDrive/data/num210.json \\\n",
        "  --validation_file /content/drive/MyDrive/data/test42.json \\\n",
        "  --output_dir /content/catalystNER \\\n",
        "  --do_train \\\n",
        "  --do_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfwMelDK39lQ"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub==0.17.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y4-v4Z_BbXA"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "# 加载模型的配置文件\n",
        "config = BertConfig.from_pretrained('pranav-s/MaterialsBERT', output_attentions=False, output_hidden_states=True)\n",
        "\n",
        "# 加载模型参数\n",
        "model = BertModel.from_pretrained('pranav-s/MaterialsBERT', config=config)\n",
        "# 加载pytorch_model.bin文件\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model_use123/pytorch_model.bin'), strict=False)\n",
        "#inputs = ('/content/drive/MyDrive/data/usener.json')\n",
        "inputs = [\"Mo-CoP\", \"requires\", \"only\", \"112\", \"and\", \"329.9\", \"mV\", \"to\", \"achieve\", \"a\", \"current\", \"density\", \"of\", \"100\", \"mA/cm^2\", \"for\", \"HER\", \"and\", \"OER\", \"in\", \"1.0\", \"M\", \"KOH,\", \"respectively.\", \"Furthermore,\", \"when\", \"it\", \"was\", \"used\", \"as\", \"bifunctional\", \"electrocatalyst,\", \"Mo-CoP\", \"could\", \"deliver\", \"10\", \"mA/cm^2\", \"at\", \"a\", \"low\", \"cell\", \"voltage\", \"of\", \"1.54\", \"V\"]\n",
        "#inputs = [None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 19, 20, 21, 22, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 34, 35, 35, 35, 35, 36, 36, 36, 36, 37, 37, 38, 39, 40, 40, 40, 41, 42, 42, 42, 42, 42, 43, 44, 44, 44, 44, 45, 46, 47, 47, 47, 47, 47, 48, 48, 49, 49, 50, 51, 52, 53, 54, 55, 55, 55, 56, 56, 57, 58, 58, 58, 59, 59, 59, 60, 61, 62, 63, 64, 64, 65, 66, 67, 68, 69, 70, 70, 70, 71, 72, 72, 73, 74, 75, 76, 76, 76, 77, 77, 77, 78, 79, 80, 81, 82, 83, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86, 87, 88, 88, 89, 90, 91, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 95, 96, 97, 98, 99, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 101, 101, 101, 102, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 104, 105, 106, 107, 107, 107, 108, 108, 108, 109, 110, 110, 110, 111, 112, 112, 113, 114, 115, 116, 117, 118, 119, 119, 119, 120, 121, 122, 123, 124, 125, 126, 127, 127, 128, 128, 128, 128, 129, 129, 129, None]\n",
        "print(inputs)\n",
        "# 预测结果\n",
        "outputs = model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbOy_s5UH1Eb"
      },
      "outputs": [],
      "source": [
        "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
        "#model_checkpoint = \"/content/drive/MyDrive/PolymerNER\"\n",
        "model_checkpoint = \"pranav-s/MaterialsBERT\"\n",
        "batch_size = 16\n",
        "truncation=True\n",
        "# 加载数据\n",
        "from datasets import load_dataset, load_metric, load_from_disk\n",
        "datasets = load_from_disk(\"/content/drive/MyDrive/autotrain-data-automatic/processed\")\n",
        "#datasets\n",
        "datasets[\"train\"][0]\n",
        "datasets[\"train\"].features[f\"tags\"]\n",
        "label_list = datasets[\"train\"].features[f\"tags\"].feature.names\n",
        "#label_list\n",
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))\n",
        "#show_random_elements(datasets[\"train\"])\n",
        "\n",
        "# 预处理数据\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
        "tokenizer(\"Hello, this is one sentence!\")\n",
        "tokenizer([\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"], is_split_into_words=True)\n",
        "example = datasets[\"train\"][4]\n",
        "print(example[\"tokens\"])\n",
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)\n",
        "len(example[f\"tags\"]), len(tokenized_input[\"input_ids\"])\n",
        "print(tokenized_input.word_ids())\n",
        "word_ids = tokenized_input.word_ids()\n",
        "aligned_labels = [-100 if i is None else example[f\"tags\"][i] for i in word_ids]\n",
        "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))\n",
        "label_all_tokens = True\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], max_length=512, truncation=True,return_tensors='pt', padding=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "tokenize_and_align_labels(datasets['train'][:5])\n",
        "#tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ozd1XQzlQCk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torchvision.transforms as transforms\n",
        "from transformers import BertModel\n",
        "from datasets import load_dataset, load_metric, load_from_disk\n",
        "\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('pranav-s/MaterialsBERT')\n",
        "        # 定义模型的layers\n",
        "        self.fc = nn.Linear(768, num_classes)  # 假设输出类别数为 num_classes\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):  # 定义模型的向前传播逻辑\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# 定义输出类别数\n",
        "num_classes = 6\n",
        "\n",
        "# 创建自定义模型实例\n",
        "model = MyModel(num_classes)\n",
        "\n",
        "# 加载预训练模型权重\n",
        "model.bert.load_state_dict(torch.load('/content/drive/MyDrive/model_use123/pytorch_model.bin'), strict=False)\n",
        "\n",
        "# 设置为评估模式\n",
        "model.eval()\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "# 加载和预处理本地数据\n",
        "#data = load_dataset('/content/drive/MyDrive/data/data_1')  # 自定义数据加载函数，将数据加载为模型可以接受的格式\n",
        "tokenizer = BertTokenizer.from_pretrained('pranav-s/MaterialsBERT')\n",
        "#inputs = tokenizer(data, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "text = [\"There\", \"is\", \"only\", \"17\", \"mV\", \"overpotential\", \"at\", \"10\", \"mA\", \"cm<sup>-2</sup>\"]\n",
        "inputs = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# 执行预测\n",
        "with torch.no_grad():\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    predictions = torch.argmax(logits, dim=1).squeeze()\n",
        "\n",
        "# 打印预测结果\n",
        "for pred in predictions:\n",
        "    print(pred.item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPLtpdPEcohR",
        "outputId": "d0ce40e4-f9d7-4738-b418-bc9da0edf8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "MazFl99jBaxL",
        "outputId": "9c6092cd-9d19-423c-ccd1-ec9f995649fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-fbbac1006ebe>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 打印预测结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 5 elements cannot be converted to Scalar"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification,BertTokenizer\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained('/content/drive/MyDrive/model_use123')\n",
        "model.eval()\n",
        "import torch\n",
        "from datasets import load_dataset, load_metric, load_from_disk\n",
        "\n",
        "# 加载和准备待标记的数据\n",
        "#data = load_dataset('/content/drive/MyDrive/data/data_1')  # 自定义数据加载函数，将数据加载为模型可以接受的格式\n",
        "tokenizer = BertTokenizer.from_pretrained('pranav-s/MaterialsBERT')\n",
        "#inputs = tokenizer(data, truncation=True, padding=True, return_tensors='pt')\n",
        "text = [\"There\", \"is\", \"only\", \"17\", \"mV\", \"overpotential\", \"at\", \"10\", \"mA\", \"cm<sup>-2</sup>\"]\n",
        "inputs = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# 执行前向传播并获取模型输出\n",
        "with torch.no_grad():\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    outputs = model(input_ids, attention_mask)\n",
        "\n",
        "\n",
        "# 提取模型输出中的预测结果\n",
        "logits = outputs.logits\n",
        "predicted_labels = torch.argmax(logits, dim=1)\n",
        "\n",
        "\n",
        "# 打印预测结果\n",
        "for label in predicted_labels:\n",
        "    print(label.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmHflWVTEamd",
        "outputId": "96dedab3-92d2-4973-80c5-2f335966a254"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained('/content/drive/MyDrive/model_use123')\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/model_use123')\n",
        "\n",
        "text = [\"There\", \"is\", \"only\", \"17\", \"mV\", \"overpotential\", \"at\", \"10\", \"mA\", \"cm<sup>-2</sup>\"]\n",
        "inputs = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# 执行前向传播并获取模型输出\n",
        "with torch.no_grad():\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    outputs = model(input_ids, attention_mask)\n",
        "\n",
        "# 提取模型输出中的预测结果\n",
        "logits = outputs.logits\n",
        "predicted_labels = torch.argmax(logits, dim=2).squeeze()\n",
        "\n",
        "# 打印预测结果\n",
        "for label in predicted_labels:\n",
        "    print(label.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvPSngkRF4CJ",
        "outputId": "86fd591f-7a69-4a6e-8f97-c83a20cf792a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/model_use123 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# 加载模型和tokenizer\n",
        "model_path = \"/content/drive/MyDrive/model_use123\"\n",
        "#tokenizer = BertTokenizer.from_pretrained(\"pranav-s/PolymerNER\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"pranav-s/MaterialsBERT\")\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# 准备输入数据\n",
        "text = [\"There\", \"is\", \"only\", \"17\", \"mV\", \"overpotential\", \"at\", \"10\", \"mA\", \"cm<sup>-2</sup>\"]\n",
        "inputs = tokenizer.encode_plus(\n",
        "    text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=512,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "input_ids = inputs.input_ids\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "# 使用模型进行预测\n",
        "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "logits = outputs.logits\n",
        "predicted_labels = torch.argmax(logits, dim=-2)\n",
        "\n",
        "# 输出预测结果\n",
        "labels = ['CATALYST', 'CO-CATALYST', 'O', 'Other', 'PROPERTY_NAME', 'PROPERTY_VALUE']  # 替换为你的标签列表\n",
        "#predicted_label = labels[predicted_labels.item()]\n",
        "#print(\"Predicted Label:\", predicted_label)\n",
        "for label in predicted_labels:\n",
        "    print(label.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zZSnRILI1vi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForTokenClassification\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained('pranav-s/PolymerNER')\n",
        "#model = BertForTokenClassification.from_pretrained('pranav-s/MaterialsBERT')\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model_use123/pytorch_model.bin'), strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYoLUgTHP1tw"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "\n",
        "model = BertModel.from_pretrained('/content/drive/MyDrive/model_use123')\n",
        "tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/model_use123')\n",
        "\n",
        "\n",
        "text = [\"There\", \"is\", \"only\", \"17\", \"mV\", \"overpotential\", \"at\", \"10\", \"mA\", \"cm<sup>-2</sup>\"]\n",
        "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "input_ids = torch.tensor(input_ids).unsqueeze(0)  # 添加批次维度\n",
        "\n",
        "model.eval()  # 设置为评估模式\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    encoded_layers = outputs[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExVxMl5WQkDK"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/drive/MyDrive/model_use123\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPCySEpxajkk",
        "outputId": "9b903b70-1255-4c1d-c077-3eda1c8c39f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'entity_group': 'PROPERTY_VALUE', 'score': 0.85398144, 'word': '17 mv', 'start': 14, 'end': 19}, {'entity_group': 'PROPERTY_NAME', 'score': 0.6716537, 'word': 'overpotential', 'start': 20, 'end': 33}, {'entity_group': 'PROPERTY_VALUE', 'score': 0.9068838, 'word': '10 ma cm', 'start': 37, 'end': 45}, {'entity_group': 'PROPERTY_VALUE', 'score': 0.6065226, 'word': 'sup', 'start': 46, 'end': 49}, {'entity_group': 'PROPERTY_VALUE', 'score': 0.6333718, 'word': '- 2', 'start': 50, 'end': 52}, {'entity_group': 'CATALYST', 'score': 0.40827653, 'word': 'pt', 'start': 114, 'end': 116}, {'entity_group': 'CATALYST', 'score': 0.4047764, 'word': 'ir', 'start': 123, 'end': 125}]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/model_use123', model_max_length=512)\n",
        "model = AutoModelForTokenClassification.from_pretrained('/content/drive/MyDrive/model_use123')\n",
        "ner_pipeline = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device='cpu')\n",
        "#text = [\"There\", \"is\", \"only\", \"17\", \"mV\", \"overpotential\", \"at\", \"10\", \"mA\", \"cm<sup>-2</sup>\"]\n",
        "text = \"There is only 17 mV overpotential at 10 mA cm<sup>-2</sup> , which is significantly lower than that of commercial Pt/C and Ir/C catalysts respectively by 26 and 3 mV\"\n",
        "#text = load_dataset('/content/drive/MyDrive/data/data_1')\n",
        "ner_output = ner_pipeline(text)\n",
        "print(ner_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EN3q1JamB0X",
        "outputId": "2b7abff1-87f3-4b69-9b88-816088057ac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Property Values: ['17 mv', '10 ma cm', 'sup', '- 2', 'sup']\n",
            "Property Names: ['overpotential']\n"
          ]
        }
      ],
      "source": [
        "data = [{'entity_group': 'PROPERTY_VALUE', 'score': 0.83087325, 'word': '17 mv', 'start': 14, 'end': 19},\n",
        "        {'entity_group': 'PROPERTY_NAME', 'score': 0.66659164, 'word': 'overpotential', 'start': 20, 'end': 33},\n",
        "        {'entity_group': 'PROPERTY_VALUE', 'score': 0.89627045, 'word': '10 ma cm', 'start': 37, 'end': 45},\n",
        "        {'entity_group': 'PROPERTY_VALUE', 'score': 0.79362094, 'word': 'sup', 'start': 46, 'end': 49},\n",
        "        {'entity_group': 'PROPERTY_VALUE', 'score': 0.66595876, 'word': '- 2', 'start': 50, 'end': 52},\n",
        "        {'entity_group': 'PROPERTY_VALUE', 'score': 0.41523603, 'word': 'sup', 'start': 54, 'end': 57}]\n",
        "\n",
        "property_values = []\n",
        "property_names = []\n",
        "\n",
        "for item in data:\n",
        "    if item['entity_group'] == 'PROPERTY_VALUE':\n",
        "        property_values.append(item['word'])\n",
        "    elif item['entity_group'] == 'PROPERTY_NAME':\n",
        "        property_names.append(item['word'])\n",
        "\n",
        "print(\"Property Values:\", property_values)\n",
        "print(\"Property Names:\", property_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymWqHptFn5c6",
        "outputId": "def6908f-8791-4c80-a517-daca6d640ce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NER output saved to: /content/drive/MyDrive/ner_output.txt\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/model_use123', model_max_length=512)\n",
        "model = AutoModelForTokenClassification.from_pretrained('/content/drive/MyDrive/model_use123')\n",
        "ner_pipeline = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device='cpu')\n",
        "\n",
        "#text = \"There is only 17 mV overpotential at 10 mA cm<sup>-2</sup>, which is significantly lower than that of commercial Pt/C and Ir/C catalysts respectively by 26 and 3 mV\"\n",
        "text = \"an efficient and durable electrocatalyst consisting of Cu_3P@NiFe-MOF is synthesized by hydrothermal method. Specially, the as-prepared Cu_3P@NiFe-MOF-4 delivery an overpotential of 226 mV at a current density of 10 mA cm^−2, and it also has a very low Tafel slope and a high double layer capacitance. \"\n",
        "ner_output = ner_pipeline(text)\n",
        "\n",
        "# Save the ner_output to a file\n",
        "output_file_path = \"/content/drive/MyDrive/ner_output.txt\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    for item in ner_output:\n",
        "        output_file.write(f\"Entity_group: {item['entity_group']}\\n\")\n",
        "        output_file.write(f\"Score: {item['score']}\\n\")\n",
        "        output_file.write(f\"Word: {item['word']}\\n\")\n",
        "        output_file.write(f\"Start: {item['start']}\\n\")\n",
        "        output_file.write(f\"End: {item['end']}\\n\")\n",
        "        output_file.write(\"\\n\")\n",
        "\n",
        "print(\"NER output saved to:\", output_file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1xmzYK0VA5Y5vXU4fkV_cHb6esT0Rcjay",
      "authorship_tag": "ABX9TyNSJ2n4ykRsCvGzjeFD/dFo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}