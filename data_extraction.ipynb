{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aTj1TzZRy1Q6-Nu0LokkpyT2WqCS-90A",
      "authorship_tag": "ABX9TyMmtpRonSnoCG490dTPqHz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qin-na/SZPT-Q/blob/main/data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFmhZMvmJ6jb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "import utils as record_extractor_utils\n",
        "import dataset\n",
        "import spacy\n",
        "\n",
        "\n",
        "import torch\n",
        "import traceback\n",
        "import argparse\n",
        "import logging\n",
        "import time\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--db_local\",\n",
        "    dest=\"db_local\",\n",
        "    help=\"True if the database to be accessed is on the same server as the server on which code is running\",\n",
        "    #  如果要访问的数据库与运行代码的服务器在同一服务器上，则为True\n",
        "    action=\"store_true\",\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--skip_n\",\n",
        "    dest=\"skip_n\",\n",
        "    help=\"Skip the first n records output from the find query\",  #  跳过查找查询输出的前n条记录\n",
        "    type=int,\n",
        "    default=0\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--cap_docs\",\n",
        "    dest=\"cap_docs\",\n",
        "    help=\"Maximum number of documents to iterate over while extracting data\", #  提取数据时要迭代的最大文档数\n",
        "    type=int,\n",
        "    default=5000\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--delete_collection\",\n",
        "    dest=\"delete_collection\",\n",
        "    help=\"True to delete any preexisting collection with the same name. False to continue adding documents to that database\",\n",
        "    #  True可删除任何先前存在的同名集合。False继续将文档添加到该数据库\n",
        "    action=\"store_true\"\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--check_repeat_doi\",\n",
        "    dest=\"check_repeat_doi\",\n",
        "    help=\"Check if the same DOI already exists in the database\",  #  检查数据库中是否已存在相同的DOI\n",
        "    action=\"store_true\"\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--collection_output_name\",\n",
        "    dest=\"collection_output_name\",\n",
        "    help=\"Name of output collection to save the data to\",  #  要将数据保存到的输出集合的名称\n",
        "    default='data_test_run'\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--use_debugpy\",\n",
        "    help=\"Use remote debugging\",  #  使用远程调试\n",
        "    action=\"store_true\",\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--verbose\",\n",
        "    help=\"Store verbose output of material_entities, group_tokens and property spans\",#  存储material_entities、group_tokens和属性跨度的详细输出\n",
        "    action=\"store_true\",\n",
        ")\n",
        "\n",
        "parser.add_argument(\n",
        "    \"--polymer_filter\",\n",
        "    help=\"Restrict extraction of data to polymer papers, the negation will look at all other papers\",\n",
        "    #  将数据提取限制在聚合物论文中，否定将着眼于所有其他论文\n",
        "    action=\"store_true\",\n",
        ")\n",
        "\n",
        "class ScaleExtraction:\n",
        "    def __init__(self, query, collection_output_name=None, skip_n=0, cap_docs=None, delete_collection=False, check_repeat_doi=False, debug=False, verbose=True, polymer_filter=True):\n",
        "        self.collection_output_name = collection_output_name\n",
        "        self.query = query\n",
        "        self.debug = debug\n",
        "        self.skip_n = skip_n\n",
        "        self.verbose = verbose\n",
        "        self.polymer_filter = polymer_filter\n",
        "        self.delete_collection = delete_collection\n",
        "        self.check_repeat_doi = check_repeat_doi\n",
        "        self.timer = {'abstract_preprocessing': [], 'ner': [], 'relation_extraction': []}\n",
        "        if cap_docs:\n",
        "            self.cap_docs = int(cap_docs)\n",
        "        else:\n",
        "            self.cap_docs = cap_docs\n",
        "        if torch.cuda.is_available():\n",
        "            print('GPU device found')\n",
        "            self.device = 1\n",
        "        else:\n",
        "            self.device = -1\n",
        "        if not self.debug:\n",
        "            self.logger = logging.getlogger(__name__)\n",
        "        else:\n",
        "            self.logger = None\n",
        "        model_file = '' # Location of BERT encoder model file to load  要加载的BERT编码器模型文件的位置\n",
        "\n",
        "        # Load NormalizationDataset used to normalize polymer names要加载的BERT编码器模型文件的位置\n",
        "        normalization_dataloader = LoadNormalizationDataset()\n",
        "        self.train_data = normalization_dataloader.process_normalization_files()\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_file, model_max_length=512)\n",
        "        model = AutoModelForTokenClassification.from_pretrained(model_file)\n",
        "        # Load model and tokenizer负载模型和令牌化器\n",
        "        self.ner_pipeline = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True, device=self.device)\n",
        "\n",
        "    def setup_connection(self):\n",
        "        \"\"\"Setup connection to a database that has stored documents. Not implemented here\"\"\"#  设置到已存储文档的数据库的连接。此处未实施\n",
        "\n",
        "        self.server = None\n",
        "        self.db = None\n",
        "        self.collection_input = None\n",
        "        if self.collection_output_name:\n",
        "            self.collection_output = self.db[self.collection_output_name]\n",
        "\n",
        "\n",
        "    def scale_data_collection(self):\n",
        "        \"\"\"Scale data collection over entire dataset\"\"\" #  将数据收集扩展到整个数据集\n",
        "        docs_parsed = self.skip_n\n",
        "        self.setup_connection()\n",
        "        num_docs = self.collection_input.count_documents(self.query)\n",
        "        print(f'Number of documents returned by query: {num_docs}')\n",
        "        if self.delete_collection:\n",
        "            self.db.drop_collection(self.collection_output)\n",
        "            print(f'Deleting collection of name {self.collection_output_name}')\n",
        "        cursor = self.collection_input.find(self.query).skip(self.skip_n)\n",
        "        if self.collection_output_name:\n",
        "            abstracts_with_data = self.collection_output.count_documents({})\n",
        "        else:\n",
        "            abstracts_with_data = 0\n",
        "        if not self.debug:\n",
        "            start_time = time.time()\n",
        "            self.logger.warning(f'Start time = {start_time}')\n",
        "        while docs_parsed < num_docs:\n",
        "            with cursor:\n",
        "                try:\n",
        "                    for i, doc in enumerate(cursor):\n",
        "\n",
        "                        doi = doc.get('DOI')\n",
        "                        if self.check_repeat_doi and self.collection_output.find_one({'DOI': doi}):\n",
        "                            continue\n",
        "                        output = {}\n",
        "                        docs_parsed+=1\n",
        "                        begin = time.time()\n",
        "                        abstract = doc['abstract']\n",
        "                        self.timer['abstract_preprocessing'].append(time.time()-begin)\n",
        "                        # Pre process abstract\n",
        "                        begin = time.time()\n",
        "                        ner_output = self.ner_pipeline(abstract, truncation=True, max_length=512)\n",
        "                        self.timer['ner'].append(time.time()-begin)\n",
        "                        if self.debug:\n",
        "                            self.ner_output = ner_output\n",
        "                            self.text = abstract\n",
        "                        # In case there are no predicted tokens, we continue to the next document\n",
        "                        if not ner_output: continue\n",
        "                        record_extraction_input = record_extractor_utils.ner_feed(ner_output, abstract)\n",
        "                        # Pass logger\n",
        "                        relation_extractor = record_extractor.RelationExtraction(text=abstract, spans=record_extraction_input, normalization_dataset=self.train_data, polymer_filter=self.polymer_filter, logger=self.logger, verbose=self.verbose)\n",
        "                        try:\n",
        "                            begin = time.time()\n",
        "                            output, _ = relation_extractor.process_document()\n",
        "                            if output:\n",
        "                                self.timer['relation_extraction'].append(time.time()-begin)\n",
        "                        except Exception as e:\n",
        "                            if not self.debug:\n",
        "                                self.logger.warning(f'Exception {e} occurred for doi {doi} while parsing the input\\n')\n",
        "                                self.logger.exception(e)\n",
        "                            else:\n",
        "                                print(f'Exception {e} occurred for doi {doi} while parsing the input\\n')\n",
        "                                print(traceback.format_exc())\n",
        "                                self.relation_extractor = relation_extractor\n",
        "                        if docs_parsed%500==0:\n",
        "                            if self.logger:\n",
        "                                self.logger.warning('\\n')\n",
        "                                self.logger.warning(f'Done with {docs_parsed} documents\\n')\n",
        "                                self.logger.warning(f'Abstracts with data: {abstracts_with_data} documents\\n')\n",
        "                                self.logger.warning(f'Positivity ratio: {float(abstracts_with_data/docs_parsed):.2f}\\n')\n",
        "                            else:\n",
        "                                print(f'\\nDone with {docs_parsed} documents\\n')\n",
        "\n",
        "                        # Log some metrics when applying model at scale  在大规模应用模型时记录一些指标\n",
        "                        if output:\n",
        "                            abstracts_with_data+=1\n",
        "                            output['DOI'] = doi\n",
        "                            output['title'] = doc.get('title')\n",
        "                            output['abstract'] = abstract\n",
        "                            output['year'] = doc.get('year', 0) # Default values in case one is not found\n",
        "                            output['month'] = doc.get('month', 0)\n",
        "                            output['day'] = doc.get('day', 0)\n",
        "                            if self.verbose:\n",
        "                                output['material_mentions'] = relation_extractor.material_entity_processor.material_mentions.return_list_dict()\n",
        "                                output['grouped_spans'] = [named_tuple_to_dict(span) for span in relation_extractor.material_entity_processor.grouped_spans]\n",
        "                            # Insert output to collection  将输出插入集合\n",
        "                            if not self.debug:\n",
        "                                self.collection_output.insert_one(output)\n",
        "                            else:\n",
        "                                print(output)\n",
        "                                self.relation_extractor = relation_extractor\n",
        "                        if self.cap_docs and i>self.cap_docs: break\n",
        "\n",
        "                except Exception as e:\n",
        "                    if self.logger:\n",
        "                        self.logger.warning(f'Exception {e} occurred for doi {doi} while iterating over cursor\\n')\n",
        "                        self.logger.exception(e)\n",
        "                    else:\n",
        "                        print(f'Exception {e} occurred for doi {doi} in outer loop\\n')\n",
        "\n",
        "            if hasattr(self, 'server'): self.server.stop()\n",
        "\n",
        "            if docs_parsed < num_docs:\n",
        "                if self.logger: self.logger.warning(f'Setting up SSH and database connection again \\n')\n",
        "                self.setup_connection()\n",
        "                cursor = self.collection_input.find(self.query).skip(docs_parsed)\n",
        "\n",
        "        if not self.debug:\n",
        "            end_time = time.time()\n",
        "            self.logger.warning(f'End time = {end_time}')\n",
        "            self.logger.warning(f'Time taken = {end_time-start_time} seconds')\n",
        "            self.logger.warning(f'Documents parsed = {docs_parsed}')\n",
        "\n",
        "def named_tuple_to_dict(named_tuple):\n",
        "    current_dict = {}\n",
        "    for col in GROUPED_SPAN_COLUMNS:\n",
        "        current_dict[col] = getattr(named_tuple, col)\n",
        "\n",
        "    return current_dict\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parser.parse_args()\n",
        "    if args.polymer_filter:\n",
        "        query = {'abstract': {'$regex': 'poly', '$options': 'i'}}\n",
        "    else:\n",
        "        query = {'$and': [{'abstract': {'$not': {'$regex': 'poly', '$options': 'i'}}}, {'abstract': {'$exists': True}}, {'abstract': {'$ne': None}}]}\n",
        "    scale_extractor = ScaleExtraction(query = query, collection_output_name=args.collection_output_name, db_local=args.db_local, skip_n = args.skip_n, cap_docs=args.cap_docs, delete_collection=args.delete_collection, check_repeat_doi=args.check_repeat_doi, debug=False, verbose=args.verbose, polymer_filter=args.polymer_filter)\n",
        "    scale_extractor.scale_data_collection()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "nf7mI_EmulAm",
        "outputId": "403e7669-25aa-4a04-b074-c4397550cecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--db_local] [--skip_n SKIP_N]\n",
            "                             [--cap_docs CAP_DOCS] [--delete_collection]\n",
            "                             [--check_repeat_doi]\n",
            "                             [--collection_output_name COLLECTION_OUTPUT_NAME]\n",
            "                             [--use_debugpy] [--verbose] [--polymer_filter]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-2cd6d967-fcf8-4394-aff8-76edcf6a2bfd.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}