{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJqFDrKyYLbVjHEg6IxXrb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qin-na/SZPT-Q/blob/main/%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOK5Wo6mpa7r",
        "outputId": "6dcfa3ec-d5f3-4983-b52a-0e03f94e2352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n",
        "print(train_images.shape)\n",
        "digit = test_images[0]\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7t-IhppKpofF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "samples = ['The cat jump over the dog', 'The dog ate my homework']\n",
        "\n",
        "#我们先将每个单词放置到一个哈希表中\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "    #将一个句子分解成多个单词\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            \n",
        "#设置句子的最大长度\n",
        "max_length = 10\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[: max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1.\n",
        "        print(\"{0} -> {1}\".format(word, results[i, j]))\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def oneHotEncode(samples):\n",
        "    #只考虑最常使用的前1000个单词\n",
        "    tokenizer = Tokenizer(num_words = 1000)\n",
        "    tokenizer.fit_on_texts(samples)\n",
        "    #把句子分解成单词数组\n",
        "    sequences = tokenizer.texts_to_sequences(samples)\n",
        "    return sequences\n",
        "\n",
        "samples = ['The cat jump over the dog', 'The dog ate my homework']\n",
        "vecs = oneHotEncode(samples)\n",
        "print(vecs)\n"
      ],
      "metadata": {
        "id": "rKsCdZyj0Eh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Conv2D, Flatten\n",
        "from keras.layers import Reshape, Conv2DTranspose\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#加载手写数字图片数据\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "image_size = x_train.shape[1]\n",
        "\n",
        "\n",
        "#把图片大小统一转换成28*28,并把像素点值都转换为[0,1]之间\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 32\n",
        "#对图片做3*3分割\n",
        "kernel_size = 3\n",
        "#让编码器将输入图片编码成含有16个元素的向量\n",
        "latent_dim = 16\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = inputs\n",
        "'''\n",
        "编码器含有两个卷积层，第一个卷积层将每个3*3切片计算成含有32个元素的向量，第二个卷积层将3*3切片\n",
        "计算成含有64个元素的向量\n",
        "'''\n",
        "\n",
        "layer_filters = [32, 64]\n",
        "for filters in layer_filters:\n",
        "  #stride=2表明每次挪到2个像素，如此一来做一次卷积运算后输出大小会减半\n",
        "  x = Conv2D(filters = filters, kernel_size = kernel_size, activation='relu',\n",
        "            strides = 2,\n",
        "            padding = 'same')(x)\n",
        "\n",
        "shape = K.int_shape(x)\n",
        "print('shape: ', shape)\n",
        "print(shape[1])\n",
        "x = Flatten()(x)\n",
        "#最后一层全连接网络输出含有16个元素的向量\n",
        "latent = Dense(latent_dim, name = 'latent_vector')(x)\n",
        "encoder = Model(inputs, latent, name='encoder')\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh3MwuDl2GVO",
        "outputId": "2c55d559-4d79-408e-91f3-3a1ade125414"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "shape:  (None, 7, 7, 64)\n",
            "7\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 14, 14, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " latent_vector (Dense)       (None, 16)                50192     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 69,008\n",
            "Trainable params: 69,008\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}