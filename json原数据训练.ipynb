{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1u81pAN96OKfHJUFUf-Hhme5fD3uc-LEP",
      "authorship_tag": "ABX9TyO7I7LGU4jY9w3PT2LWFU2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qin-na/SZPT-Q/blob/main/json%E5%8E%9F%E6%95%B0%E6%8D%AE%E8%AE%AD%E7%BB%83.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "157Sk2S3T1GR",
        "outputId": "ecec9cbe-e6c4-4107-ab1c-974cf834c650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 159264, done.\u001b[K\n",
            "remote: Counting objects: 100% (1715/1715), done.\u001b[K\n",
            "remote: Compressing objects: 100% (963/963), done.\u001b[K\n",
            "remote: Total 159264 (delta 1033), reused 1214 (delta 678), pack-reused 157549\u001b[K\n",
            "Receiving objects: 100% (159264/159264), 160.30 MiB | 21.41 MiB/s, done.\n",
            "Resolving deltas: 100% (119239/119239), done.\n",
            "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!cd transformers\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1eeIMCTXUTF",
        "outputId": "552c36ed-b4e2-49a6-c5aa-51af4c17fe1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets>=1.4.0 (from -r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1))\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/519.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/519.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m512.0/519.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (2.13.0)\n",
            "Collecting evaluate>=0.2.0 (from -r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 3))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (4.66.1)\n",
            "Collecting xxhash (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (0.33.0)\n",
            "Collecting responses<0.19 (from evaluate>=0.2.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 3))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (0.41.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (2023.7.22)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (2.3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.4.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 1)) (2023.3.post1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.3.0->-r /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/requirements.txt (line 2)) (3.2.2)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, huggingface-hub, datasets, evaluate\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 evaluate-0.4.0 huggingface-hub-0.17.1 multiprocess-0.70.15 responses-0.18.0 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbm1vmTqXu5Z",
        "outputId": "252c19f1-379d-4514-8a9c-6fb1a26e8620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, transformers\n",
            "Successfully installed safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/run_ner.py \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --dataset_name conll2003 \\\n",
        "  --output_dir /tmp/test-ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fddQLNsSWO1E",
        "outputId": "bcb66e20-2557-4a3b-9ba7-9ca00dd8d689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-12 07:54:31.432765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.33.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.33.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.33.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Running tokenizer on dataset: 100% 3250/3250 [00:00<00:00, 7149.14 examples/s]\n",
            "Sample 10476 of the training set: {'input_ids': [101, 7797, 2012, 5169, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1], 'labels': [-100, 3, 0, 5, -100]}.\n",
            "Sample 1824 of the training set: {'input_ids': [101, 4262, 2007, 3607, 1010, 2029, 2003, 2256, 2364, 4256, 1010, 2031, 2307, 5197, 1010, 1000, 13970, 2818, 2863, 2056, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100, -100, 0, 0, -100]}.\n",
            "Sample 409 of the training set: {'input_ids': [101, 2002, 2794, 1024, 1000, 2065, 2053, 2028, 2356, 1010, 1045, 2196, 2441, 2026, 2677, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}.\n",
            "Tensorflow: setting up strategy\n",
            "2023-09-12 07:54:36.185654: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/model.safetensors\n",
            "Loaded 108,891,648 parameters in the TF 2.0 model.\n",
            "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss. You can also specify `loss='auto'` to get the internal loss without printing this info string.\n",
            "***** Running training *****\n",
            "  Num examples = 14041\n",
            "  Num Epochs = 3.0\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size = 8\n",
            "Epoch 1/3\n",
            "1755/1755 [==============================] - 320s 151ms/step - loss: 0.0915 - val_loss: 0.0517\n",
            "Epoch 2/3\n",
            "1755/1755 [==============================] - 213s 121ms/step - loss: 0.0270 - val_loss: 0.0535\n",
            "Epoch 3/3\n",
            "1755/1755 [==============================] - 212s 121ms/step - loss: 0.0113 - val_loss: 0.0530\n",
            "407/407 [==============================] - 22s 46ms/step\n",
            "Evaluation metrics:\n",
            "precision: 0.9442\n",
            "recall: 0.9517\n",
            "f1: 0.9480\n",
            "accuracy: 0.9894\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/transformers/examples/tensorflow/token-classification/run_ner.py\", line 647, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/transformers/examples/tensorflow/token-classification/run_ner.py\", line 637, in main\n",
            "    with open(output_eval_file, \"w\") as writer:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/test-ner/all_results.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0CCQ_0vYeq6",
        "outputId": "0dbcc5f2-e6d6-42e7-ccb6-f8640ab142fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=00bbe3fbba97eb1ab384041ed2a9f4f410d48d17f32ce39214c07f23e3b00d1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/run_ner.py \\\n",
        "  --model_name_or_path pranav-s/PolymerNER \\\n",
        "  --dataset_name QNN/autotrain-data-automatic \\\n",
        "  --output_dir /tmp/test-ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172ed016-3a9f-43be-fb0f-1870a45ff97d",
        "id": "P-0A7LF3dIpy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-12 08:16:10.723908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading readme: 100% 10.2k/10.2k [00:00<00:00, 34.2MB/s]\n",
            "Downloading data files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading data: 100% 666/666 [00:00<00:00, 7.73kB/s]\n",
            "\n",
            "Downloading data: 100% 273/273 [00:00<00:00, 3.23kB/s]\n",
            "Downloading data files:  50% 1/2 [00:00<00:00,  5.73it/s]\n",
            "Downloading data: 100% 665/665 [00:00<00:00, 7.75kB/s]\n",
            "\n",
            "Downloading data: 100% 273/273 [00:00<00:00, 3.20kB/s]\n",
            "Downloading data files: 100% 2/2 [00:00<00:00,  5.73it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1290.75it/s]\n",
            "Generating train split: 1 examples [00:00, 240.72 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1940, in _prepare_split_single\n",
            "    writer.write_table(table)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\", line 572, in write_table\n",
            "    pa_table = table_cast(pa_table, self._schema)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/table.py\", line 2328, in table_cast\n",
            "    return cast_table_to_schema(table, schema)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/table.py\", line 2286, in cast_table_to_schema\n",
            "    raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nbecause column names don't match\")\n",
            "ValueError: Couldn't cast\n",
            "_data_files: list<item: struct<filename: string>>\n",
            "  child 0, item: struct<filename: string>\n",
            "      child 0, filename: string\n",
            "_fingerprint: string\n",
            "_format_columns: list<item: string>\n",
            "  child 0, item: string\n",
            "_format_kwargs: struct<>\n",
            "_format_type: null\n",
            "_output_all_columns: bool\n",
            "_split: null\n",
            "to\n",
            "{'citation': Value(dtype='string', id=None), 'description': Value(dtype='string', id=None), 'features': {'tokens': {'feature': {'dtype': Value(dtype='string', id=None), '_type': Value(dtype='string', id=None)}, '_type': Value(dtype='string', id=None)}, 'tags': {'feature': {'names': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), '_type': Value(dtype='string', id=None)}, '_type': Value(dtype='string', id=None)}}, 'homepage': Value(dtype='string', id=None), 'license': Value(dtype='string', id=None), 'splits': {'train': {'name': Value(dtype='string', id=None), 'num_bytes': Value(dtype='int64', id=None), 'num_examples': Value(dtype='int64', id=None), 'dataset_name': Value(dtype='null', id=None)}}}\n",
            "because column names don't match\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/transformers/examples/tensorflow/token-classification/run_ner.py\", line 647, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/transformers/examples/tensorflow/token-classification/run_ner.py\", line 251, in main\n",
            "    raw_datasets = load_dataset(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2153, in load_dataset\n",
            "    builder_instance.download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 954, in download_and_prepare\n",
            "    self._download_and_prepare(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1049, in _download_and_prepare\n",
            "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1813, in _prepare_split\n",
            "    for job_id, done, content in self._prepare_split_single(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1958, in _prepare_split_single\n",
            "    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\n",
            "datasets.builder.DatasetGenerationError: An error occurred while generating the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/run_ner.py \\\n",
        "  --model_name_or_path pranav-s/MaterialsBERT \\\n",
        "  --train_file /content/drive/MyDrive/data/train126.json \\\n",
        "  --validation_file /content/drive/MyDrive/data/validation42.json \\\n",
        "  --output_dir /tmp/test-ner"
      ],
      "metadata": {
        "id": "FBx2qFDleIF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tRdpY5uRsOzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/transformers/examples/tensorflow/token-classification/run_ner.py \\\n",
        "  --model_name_or_path pranav-s/MaterialsBERT \\\n",
        "  --train_file /content/drive/MyDrive/data/train126.json \\\n",
        "  --validation_file /content/drive/MyDrive/data/validation42.json \\\n",
        "  --output_dir /content/drive/MyDrive/results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859ae426-6f7d-46ce-a8b8-da858475a0fd",
        "id": "DqzmN9F2sPHo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-12 09:24:35.668579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.33.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.33.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.33.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"pranav-s/MaterialsBERT\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.33.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Running tokenizer on dataset: 100% 42/42 [00:00<00:00, 442.92 examples/s]\n",
            "Sample 81 of the training set: {'input_ids': [2, 12956, 6201, 4854, 3227, 12, 3656, 13, 12715, 1956, 3626, 6804, 4075, 1922, 10746, 2961, 1977, 3311, 1966, 4550, 1958, 3224, 18105, 18, 1922, 2052, 2161, 16, 43, 3335, 1930, 5288, 2417, 1982, 3530, 1942, 12674, 16731, 17, 3189, 13299, 17, 20773, 17, 28470, 22467, 2457, 17487, 9788, 2331, 8648, 2062, 12, 15611, 1924, 1032, 7312, 13, 2007, 2193, 6185, 2783, 2261, 1930, 14184, 2199, 1997, 28621, 23713, 9140, 12, 19038, 1030, 13, 1966, 1920, 2081, 17, 3170, 17, 23258, 4820, 18, 5580, 1941, 2037, 1920, 4346, 8518, 1930, 11129, 3949, 3742, 15926, 16, 1920, 1966, 17, 4649, 15611, 1924, 1032, 7312, 5822, 10877, 4256, 13142, 18914, 3604, 1958, 1920, 3656, 1922, 21, 18, 20, 55, 16335, 16, 1956, 43, 2330, 2338, 13067, 2514, 12, 166, 16, 2950, 5963, 13, 1942, 9265, 2119, 2488, 3195, 32, 2302, 34, 17, 22, 32, 19, 2302, 34, 16, 2858, 19063, 1938, 9096, 12, 4182, 5963, 2344, 32, 2302, 34, 17, 21, 32, 19, 2302, 34, 13, 16, 1930, 2149, 6818, 3214, 3824, 12, 52, 32, 2316, 34, 20, 32, 19, 2316, 34, 16, 23, 18, 2627, 2488, 3195, 32, 2302, 34, 17, 22, 32, 19, 2302, 34, 13, 2409, 1920, 2815, 17, 2576, 12472, 8362, 18, 1920, 1966, 17, 3530, 4884, 10710, 1036, 2673, 8619, 5522, 1958, 18639, 5570, 11129, 3949, 3742, 17201, 1958, 3372, 1927, 6201, 1922, 16037, 2094, 18, 32, 19, 58, 34, 32, 58, 34, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, -100, -100, -100, -100, -100, -100, 2, 2, -100, -100, -100, 2, -100, -100, -100, 2, -100, 2, 2, 2, -100, -100, 2, 2, -100, -100, -100, -100, 2, 2, -100, -100, -100, 2, 2, 2, -100, -100, -100, -100, 2, -100, 2, -100, 2, 2, 2, 2, 2, 2, -100, -100, 2, -100, 2, 2, -100, -100, 0, -100, -100, 0, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, -100, 2, 2, 2, 3, -100, -100, 2, -100, -100, 4, 4, 2, 2, 2, 4, 4, 4, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, 2, 3, -100, 2, 2, 4, 4, 4, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, -100, 2, 2, 2, 3, 3, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4, -100, -100, 4, 4, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, 2, 2, 2, -100, -100, 2, -100, -100, 2, 2, -100, -100, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, -100, -100, -100, 2, -100, -100, -100]}.\n",
            "Sample 14 of the training set: {'input_ids': [2, 6201, 4854, 3227, 12, 3656, 13, 2037, 3224, 2596, 13142, 4607, 4662, 2193, 4455, 17, 3655, 4751, 1942, 14732, 3648, 4836, 4401, 17201, 14707, 4733, 13353, 1958, 8901, 3844, 6233, 18, 1922, 2052, 2778, 2038, 3530, 43, 3626, 3500, 1930, 4700, 12715, 3165, 2081, 17078, 13151, 7437, 5680, 6487, 2394, 18097, 12, 22, 13, 9692, 26618, 17641, 1956, 5056, 20036, 12, 2650, 12, 21, 17, 66, 13, 6374, 1036, 12, 22, 13, 19, 15760, 5249, 17201, 13, 1958, 3656, 1922, 9931, 6551, 18, 1920, 6487, 2394, 3193, 1927, 2650, 12, 21, 17, 66, 13, 6374, 1036, 12, 22, 13, 19, 15760, 1982, 4378, 2007, 10201, 66, 17, 6451, 11530, 1930, 6726, 8517, 18, 13197, 4146, 2594, 43, 2330, 2338, 13067, 2514, 1927, 363, 20, 18, 2369, 64, 2019, 2036, 2488, 19, 3195, 12, 22, 13, 16, 2858, 19063, 1938, 9096, 1927, 363, 4182, 5963, 19, 10468, 16, 1930, 2815, 17, 2576, 26916, 2338, 3040, 50, 1927, 3656, 7277, 2193, 9390, 12603, 1927, 2650, 12, 20, 18, 29, 13, 2081, 12, 20, 18, 21, 13, 61, 12, 22, 13, 19, 15760, 5249, 17201, 2019, 2149, 18908, 12, 363, 1027, 2786, 19, 3195, 12, 22, 13, 13, 18, 3824, 3183, 6460, 8907, 3404, 1988, 1920, 4853, 1927, 2149, 6804, 2455, 4402, 2947, 2037, 43, 3103, 3378, 1927, 1920, 9010, 3844, 7104, 1927, 50, 6761, 9724, 1990, 18097, 12, 22, 13, 3154, 4073, 2081, 20537, 1922, 1920, 5680, 6487, 2394, 3170, 18, 2176, 1977, 2222, 2435, 1988, 1920, 2149, 3656, 6804, 2455, 1927, 2650, 12, 20, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, -100, -100, 2, 2, -100, -100, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, -100, -100, -100, 2, -100, 2, 2, 2, 2, 2, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, 2, -100, 2, 2, 2, 2, 2, -100, 2, 2, -100, 2, 2, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 3, -100, -100, 2, 2, 4, -100, -100, 4, 2, 4, 4, -100, -100, -100, -100, -100, 2, 2, 3, -100, 2, 2, 2, 4, 4, -100, -100, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, 2, 2, 2, 2, 2, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100]}.\n",
            "Sample 3 of the training set: {'input_ids': [2, 2038, 8040, 1930, 4482, 19681, 2260, 17, 5759, 28843, 9286, 14578, 1915, 5859, 1966, 17201, 1958, 1920, 6201, 4854, 3227, 12, 3656, 13, 18, 10058, 1990, 2260, 3112, 6346, 16, 2038, 7569, 43, 4080, 2561, 17, 10270, 2161, 1927, 1920, 3656, 2585, 1990, 2052, 5759, 36, 2260, 12773, 24254, 21259, 2381, 2298, 18, 2256, 7426, 1920, 6201, 17, 11709, 8513, 3360, 2441, 2970, 2961, 1966, 2260, 3112, 6346, 9169, 1009, 16, 3656, 3227, 12887, 1930, 8304, 2032, 4396, 2222, 2710, 7596, 2564, 2193, 2321, 14130, 1930, 9500, 3130, 18, 2038, 5387, 1988, 2260, 3112, 6346, 1977, 43, 4227, 6610, 1966, 43, 3656, 12715, 16, 1956, 4227, 5057, 1930, 1925, 3399, 3227, 3844, 7104, 1927, 20, 18, 5792, 2180, 1966, 1925, 5199, 4124, 18, 2038, 2222, 6358, 11615, 1958, 1920, 3036, 1927, 3656, 28843, 9286, 14578, 1915, 17201, 18, 32, 19, 58, 34, 32, 58, 34, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 2, 2, 2, 2, 2, 0, -100, -100, 0, -100, -100, -100, 0, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, 2, 2, 0, -100, -100, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, -100, -100, -100, -100, -100, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 0, -100, -100, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 2, 0, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, -100, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, 2, -100, 2, -100, -100, -100, 2, -100, -100, -100]}.\n",
            "Tensorflow: setting up strategy\n",
            "2023-09-12 09:24:39.607289: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin\n",
            "Loading PyTorch weights from /root/.cache/huggingface/hub/models--pranav-s--MaterialsBERT/snapshots/8a7378e9cdeae17fb808482ae367e4bd995fe223/pytorch_model.bin\n",
            "PyTorch checkpoint contains 132,986,228 parameters\n",
            "Loaded 108,891,648 parameters in the TF 2.0 model.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss. You can also specify `loss='auto'` to get the internal loss without printing this info string.\n",
            "***** Running training *****\n",
            "  Num examples = 126\n",
            "  Num Epochs = 3.0\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size = 8\n",
            "Epoch 1/3\n",
            "15/15 [==============================] - 67s 915ms/step - loss: 0.5194 - val_loss: 0.2632\n",
            "Epoch 2/3\n",
            "15/15 [==============================] - 8s 534ms/step - loss: 0.1476 - val_loss: 0.1809\n",
            "Epoch 3/3\n",
            "15/15 [==============================] - 7s 495ms/step - loss: 0.0978 - val_loss: 0.1742\n",
            "6/6 [==============================] - 5s 143ms/step\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CATALYST seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPERTY_VALUE seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPERTY_NAME seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CO-CATALYST seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Evaluation metrics:\n",
            "precision: 0.7238\n",
            "recall: 0.6484\n",
            "f1: 0.6841\n",
            "accuracy: 0.9591\n",
            "Configuration saved in /content/drive/MyDrive/results/config.json\n",
            "Model weights saved in /content/drive/MyDrive/results/tf_model.h5\n"
          ]
        }
      ]
    }
  ]
}